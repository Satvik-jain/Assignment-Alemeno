{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6b946dc-96ee-4e77-b41d-c9dbd423acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain-community \n",
    "# !pip install pinecone\n",
    "# !pip install unstructured[pdf]\n",
    "# !pip install libmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b01a03-f1b9-4bd1-bc4e-0dc097c0388a",
   "metadata": {},
   "source": [
    "## Data Loading and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b619e5-c487-4801-951c-1421c87ab94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.11\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8e29e71-4054-4c2b-8650-41c798dfca24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buddha/Desktop/Satvik/assign/venv/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "import langchain_community.vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af82fb60-cfd9-4dd0-a494-9d101eeb5505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:35<00:00, 11.79s/it]\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=600\n",
    "        )\n",
    "loader = DirectoryLoader('knowledge_base', glob=\"*.pdf\", show_progress=True, use_multithreading=True)\n",
    "data = loader.load()\n",
    "\n",
    "for doc in data:\n",
    "    doc.metadata['source'] = doc.metadata.get('source', 'Unknown')\n",
    "\n",
    "docs = text_splitter.split_documents(data)\n",
    "document_texts = [doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f80d2ed-e0c0-4401-8b53-8ae816653cdf",
   "metadata": {},
   "source": [
    "## VectorStore and Query Engine Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "990e1b9b-2756-4628-b8ad-4546961fa1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5761711e-e9dc-4edd-9d37-21c1a5261280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1585065/3327656224.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "import pinecone\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "# pc = Pinecone(api_key = secret_value_0)\n",
    "index_name = \"internship-assignment\"\n",
    "\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(name=index_name,\n",
    "                    metric=\"cosine\",\n",
    "                    dimension=768,\n",
    "                    spec=ServerlessSpec(\n",
    "                        cloud=\"aws\",\n",
    "                        region=\"us-east-1\"\n",
    "                ))\n",
    "    docsearch = langchain_community.vectorstores.Pinecone.from_documents(docs, embeddings, index_name=index_name)\n",
    "else:\n",
    "    docsearch = langchain_community.vectorstores.Pinecone.from_existing_index(embedding=embeddings, index_name=index_name)\n",
    "\n",
    "print(\"Pipeline setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e8587c5-84e9-4704-bd05-f803931fbe85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.pinecone.Pinecone at 0x7ed26a847d90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fefc7b-f5f5-4f7d-8a54-055da1ba9570",
   "metadata": {},
   "source": [
    "## ChatBot and LLM init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "018874a7-d018-47d6-b345-7641bee0ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering.chain import load_qa_chain\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06b6894e-a057-4cee-abaf-e84724c7bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM():\n",
    "    def __init__(self, prompt=\"Hi\"):\n",
    "        self.clear_memory = True\n",
    "        self.model = None\n",
    "        self.chat_history = []\n",
    "        self.prompt = None\n",
    "        self.memory = None\n",
    "        self.template = \"\"\" \n",
    "     You are a helpful chatbot.\n",
    "     You are provided with information from these 3 documents\n",
    "    1. Alphabet Inc. Form 10-K\n",
    "    2. Tesla, Inc. Form 10-K\n",
    "    3. Uber Technologies, Inc. Form 10-K\n",
    "    Your task is to retrieve the content from these PDFs, compare\n",
    "    them, and answer queries highlighting the information across all\n",
    "    documents.\n",
    "     Use the provided context to answer the user's question accurately. Always consider the user's chat history for better understanding and personalized responses.\n",
    "                            Here is the information you have:\n",
    "    \n",
    "                            Context: \n",
    "                            {context}\n",
    "    \n",
    "                            Chat History: \n",
    "                            {chat_history}\n",
    "    \n",
    "                            User's Question: \n",
    "                            {question}\n",
    "    \n",
    "                            Based on the above information, provide a detailed and accurate answer to the user's question. Remember to stay relevant to the context and maintain professionalism. Your response should be clear, concise, and helpful:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def Ollama(self, given_prompt):\n",
    "        user_input = given_prompt\n",
    "        if self.clear_memory:\n",
    "            self.model = OllamaLLM(model=\"llama3.2:3b\", temprature = 0.7)\n",
    "            template = self.template\n",
    "            self.prompt = PromptTemplate(\n",
    "                template=template,\n",
    "                input_variables=[\"chat_history\", \"context\", \"question\"]\n",
    "            )\n",
    "            self.memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\")\n",
    "            self.clear_memory = False\n",
    "        chain = load_qa_chain(prompt = self.prompt, llm = self.model, memory = self.memory, chain_type = \"stuff\")\n",
    "        print(chain.memory.buffer)\n",
    "        return chain({\n",
    "                        \"input_documents\": docsearch.similarity_search(user_input),\n",
    "                        \"question\": user_input\n",
    "                    }, \n",
    "                        return_only_outputs=True\n",
    "                    )[\"output_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "266c0697-79df-4a64-a446-86b9683d1d8e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# class Ollama():\n",
    "#     def __init__(self):\n",
    "#         self.clear_memory = True\n",
    "    \n",
    "#     def ollamachat(self,given_prompt):\n",
    "#         user_input = given_prompt\n",
    "#         if self.clear_memory:\n",
    "#             model = OllamaLLM(model=\"llama3.2:3b\", temprature = 0.7)\n",
    "#             template = \"\"\" \n",
    "#      You are a helpful chatbot.\n",
    "#      You are provided with information from these 3 documents\n",
    "#     1. Alphabet Inc. Form 10-K\n",
    "#     2. Tesla, Inc. Form 10-K\n",
    "#     3. Uber Technologies, Inc. Form 10-K\n",
    "#     Your task is to retrieve the content from these PDFs, compare\n",
    "#     them, and answer queries highlighting the information across all\n",
    "#     documents.\n",
    "#      Use the provided context to answer the user's question accurately. Always consider the user's chat history for better understanding and personalized responses.\n",
    "#                             Here is the information you have:\n",
    "    \n",
    "#                             Context: \n",
    "#                             {context}\n",
    "    \n",
    "#                             Chat History: \n",
    "#                             {chat_history}\n",
    "    \n",
    "#                             User's Question: \n",
    "#                             {question}\n",
    "    \n",
    "#                             Based on the above information, provide a detailed and accurate answer to the user's question. Remember to stay relevant to the context and maintain professionalism. Your response should be clear, concise, and helpful:\n",
    "#     \"\"\"\n",
    "#             prompt = PromptTemplate(\n",
    "#                 template=template,\n",
    "#                 input_variables=[\"chat_history\", \"context\", \"question\"]\n",
    "#             )\n",
    "#             memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\")\n",
    "#             self.clear_memory = False\n",
    "#         chain = load_qa_chain(prompt = prompt, llm = model, memory = memory, chain_type = \"stuff\")\n",
    "#         print(chain.memory.buffer)\n",
    "#         return chain({\n",
    "#                         \"input_documents\": docsearch.similarity_search(user_input),\n",
    "#                         \"question\": user_input\n",
    "#                     }, \n",
    "#                         return_only_outputs=True\n",
    "#                     )[\"output_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3985f67-fc9f-4998-a30d-13dc554f1e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "given_prompt = \"What is the total revenue for Google Search?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b835bde-8ee2-4905-ad52-d526052ce46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm  =  LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10d5214e-8975-4b94-885c-426d1b58a678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1585065/3120765430.py:42: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  self.memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\")\n",
      "/tmp/ipykernel_1585065/3120765430.py:44: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
      "  chain = load_qa_chain(prompt = self.prompt, llm = self.model, memory = self.memory, chain_type = \"stuff\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1585065/3120765430.py:46: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return chain({\n"
     ]
    }
   ],
   "source": [
    "output = llm.Ollama(given_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f425afb-ae71-4b90-82c1-b957868ff2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information from Alphabet Inc., Form 10-K, I can answer your question about total revenue for Google Search.\n",
      "\n",
      "According to the table presented in the financial results section of Alphabet Inc.'s Form 10-K, the total revenue for Google Search & other is:\n",
      "\n",
      "- $148,951 (2021)\n",
      "- $162,450 (2022)\n",
      "- $175,033 (2023)\n",
      "\n",
      "However, if we are looking for the specific number representing the total revenue for Google Search itself, it seems that there isn't a separate line item labeled as such in the provided financial reports.\n",
      "\n",
      "But since the question pertains to the revenues from \"Google Search & other\", which includes both search-related and other services (e.g., YouTube ads), we can infer its impact on the overall revenue. The total increase in this segment from 2021 to 2023 is $12.6 billion, indicating significant growth.\n",
      "\n",
      "If you're looking for a more precise figure related specifically to Google Search or want to explore other aspects of Alphabet's financials, I recommend reviewing their financial reports and summaries for further insights.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cc0d14-c861-4ffd-b2fb-07bc60c7efcc",
   "metadata": {},
   "source": [
    "## Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46522127-9726-4895-ab1f-0d8850fcf032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9b80375-acd8-4526-af1f-55c5684f7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9db9d498-037f-47dc-b94f-650adb7596f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62f3fc0f-32fc-4b7b-859e-0629c56a57a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_output(prompt):\n",
    "        llm.chat_history.append([prompt, llm.Ollama(prompt)])\n",
    "        return llm.chat_history\n",
    "\n",
    "def clear_fn():\n",
    "    llm.clear_memory = True\n",
    "    llm.chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c5bab6e-65d8-483a-ab38-58f67eb78c1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buddha/Desktop/Satvik/assign/venv/lib/python3.11/site-packages/gradio/components/chatbot.py:237: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks(fill_height = True) as app:\n",
    "    with gr.Tab(\"📃 Comparative Analysis of Financial Reports\"):\n",
    "        gr.Markdown('''## 🤖 This application is developed as part of an assignment for an internship at Alemeno. \n",
    "                          It allows you to analyze and compare Form 10-K filings of multinational companies, including \n",
    "                          - Alphabet Inc.,\n",
    "                          - Tesla Inc., and\n",
    "                          - Uber Technologies Inc.\n",
    "                        ''')\n",
    "        with gr.Column():\n",
    "            with gr.Column(scale = 25):\n",
    "                with gr.Group():\n",
    "                    chatbox = gr.Chatbot(label = \"🔭 ChatBot Panel\", show_copy_button = True, height=480)\n",
    "                    textbox = gr.Textbox(show_label = False, placeholder = \"👉 Enter your query\")\n",
    "\n",
    "                    textbox.submit(\n",
    "                        fn = gen_output,\n",
    "                        inputs = textbox,\n",
    "                        outputs = chatbox\n",
    "                    )\n",
    "                    submit_button = gr.Button(\"Submit\")\n",
    "                    submit_button.click(\n",
    "                        fn = gen_output,\n",
    "                        inputs = textbox,\n",
    "                        outputs = chatbox\n",
    "                    )\n",
    "\n",
    "                clear = gr.ClearButton([textbox, chatbox],value = \"Clear Memory and Start New Chat\")\n",
    "                clear.click(fn= clear_fn)\n",
    "        \n",
    "app.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ee55c-5f35-4d46-8298-365f8f20de7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13b3b0e-2342-4d00-8b33-c5d10b846a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6292478,
     "sourceId": 10185751,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3.11.12 (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
